"""
ì¥í•™ê¸ˆ ì •ë³´ í¬ë¡¤ë§ ë° ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
Supabase DBì— ìë™ìœ¼ë¡œ ì¥í•™ê¸ˆ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
"""

import os
import json
import time
import re
from datetime import datetime
from typing import Dict, List, Optional
from dotenv import load_dotenv

import requests
from bs4 import BeautifulSoup
from openai import OpenAI
from supabase import create_client, Client

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_KEY")
)

# ì„¤ì •ê°’
TARGET_URL = os.getenv("TARGET_URL")
MAX_PAGES = int(os.getenv("MAX_PAGES", "10"))
DELAY_SECONDS = int(os.getenv("DELAY_SECONDS", "2"))


class ScholarshipCrawler:
    """ì¥í•™ê¸ˆ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.processed_links = set()
    
    def crawl_scholarship_list(self, url: str) -> List[Dict]:
        """
        ì¥í•™ê¸ˆ ê²Œì‹œíŒ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ê³µê³  ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        Args:
            url: í¬ë¡¤ë§í•  ê²Œì‹œíŒ URL
            
        Returns:
            ì¥í•™ê¸ˆ ê³µê³  ë¦¬ìŠ¤íŠ¸ [{'title': str, 'link': str, 'raw_html': str}, ...]
        """
        try:
            print(f"ğŸ“¡ í¬ë¡¤ë§ ì‹œì‘: {url}")
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'lxml')
            scholarships = []
            
            # class="detailLink"ë¥¼ ê°€ì§„ ëª¨ë“  ë§í¬ ì°¾ê¸°
            detail_links = soup.find_all('a', class_='detailLink')
            
            print(f"âœ… {len(detail_links)}ê°œì˜ ê³µê³ ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
            
            for link in detail_links:
                title = link.get_text(strip=True)
                
                # ë§í¬ URL ì¶”ì¶œ (ì‹¤ì œ ì‚¬ì´íŠ¸ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
                # data-paramsì—ì„œ ì‹¤ì œ ë§í¬ë¥¼ êµ¬ì„±í•˜ê±°ë‚˜, href ì†ì„± ì‚¬ìš©
                data_params = link.get('data-params', '')
                href = link.get('href', '#')
                
                # ì‹¤ì œ ìƒì„¸ í˜ì´ì§€ URL êµ¬ì„±
                if href and href != '#':
                    detail_url = self._build_full_url(url, href)
                else:
                    # data-paramsì—ì„œ URL êµ¬ì„± (ì‚¬ì´íŠ¸ë³„ë¡œ ë‹¤ë¦„)
                    detail_url = self._extract_detail_url(url, data_params)
                
                if detail_url and detail_url not in self.processed_links:
                    scholarships.append({
                        'title': title,
                        'link': detail_url,
                        'data_params': data_params
                    })
                    self.processed_links.add(detail_url)
            
            return scholarships
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return []
    
    def crawl_scholarship_detail(self, url: str) -> Optional[str]:
        """
        ì¥í•™ê¸ˆ ìƒì„¸ í˜ì´ì§€ì˜ ë³¸ë¬¸ ë‚´ìš©ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
        
        Args:
            url: ìƒì„¸ í˜ì´ì§€ URL
            
        Returns:
            ë³¸ë¬¸ í…ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ None)
        """
        try:
            print(f"  ğŸ“„ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§: {url}")
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ (ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
            # ì¼ë°˜ì ì¸ ê²Œì‹œíŒ êµ¬ì¡° ì‹œë„
            content = None
            
            # ë°©ë²• 1: classë‚˜ idë¡œ ë³¸ë¬¸ ì°¾ê¸°
            content_div = soup.find('div', class_=re.compile(r'content|article|post|body', re.I))
            if content_div:
                content = content_div.get_text(strip=True, separator='\n')
            
            # ë°©ë²• 2: íŠ¹ì • íƒœê·¸ ì°¾ê¸°
            if not content:
                article = soup.find('article')
                if article:
                    content = article.get_text(strip=True, separator='\n')
            
            # ë°©ë²• 3: ì „ì²´ bodyì—ì„œ ì¶”ì¶œ
            if not content:
                body = soup.find('body')
                if body:
                    # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                    for tag in body(['script', 'style', 'nav', 'header', 'footer']):
                        tag.decompose()
                    content = body.get_text(strip=True, separator='\n')
            
            return content if content else None
            
        except Exception as e:
            print(f"  âŒ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None
    
    def _build_full_url(self, base_url: str, href: str) -> str:
        """ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜"""
        from urllib.parse import urljoin
        return urljoin(base_url, href)
    
    def _extract_detail_url(self, base_url: str, data_params: str) -> Optional[str]:
        """data-paramsì—ì„œ ìƒì„¸ í˜ì´ì§€ URL ì¶”ì¶œ (ì‚¬ì´íŠ¸ë³„ ì»¤ìŠ¤í„°ë§ˆì´ì§• í•„ìš”)"""
        try:
            # data_paramsê°€ JSON í˜•íƒœì¸ ê²½ìš°
            params = json.loads(data_params.replace('&quot;', '"'))
            
            # ì˜ˆì‹œ: íŒŒë¼ë¯¸í„°ë¡œ URL êµ¬ì„±
            # ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”
            if 'encMenuSeq' in params and 'encMenuBoardSeq' in params:
                detail_url = f"{base_url}?seq={params['encMenuBoardSeq']}"
                return detail_url
            
            return None
        except:
            return None


class GPTAnalyzer:
    """GPT-4o-minië¥¼ ì‚¬ìš©í•œ ì¥í•™ê¸ˆ ì •ë³´ ë¶„ì„"""
    
    @staticmethod
    def analyze_scholarship(title: str, content: str) -> Optional[Dict]:
        """
        GPT-4o-minië¥¼ ì‚¬ìš©í•˜ì—¬ ì¥í•™ê¸ˆ ì •ë³´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
        
        Args:
            title: ì¥í•™ê¸ˆ ì œëª©
            content: ì¥í•™ê¸ˆ ë³¸ë¬¸ ë‚´ìš©
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
        """
        try:
            print(f"  ğŸ¤– GPT ë¶„ì„ ì¤‘...")
            
            # GPTì—ê²Œ ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸
            system_prompt = """ë‹¹ì‹ ì€ ì¥í•™ê¸ˆ ê³µê³ ë¬¸ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì¥í•™ê¸ˆ ê³µê³ ì—ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSON í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”.

ì¶”ì¶œí•  ì •ë³´:
1. min_gpa: ìµœì†Œ í•™ì  ìš”êµ¬ì‚¬í•­ (ì—†ìœ¼ë©´ 0.0)
2. max_income: ì†Œë“ë¶„ìœ„ ìƒí•œì„  (0~10, ì œí•œ ì—†ìœ¼ë©´ 99)
3. residence: ê±°ì£¼ì§€ ì œí•œ (ì˜ˆ: 'ì„œìš¸', 'ê²½ê¸°', 'ì „êµ­' ë“±, ì œí•œ ì—†ìœ¼ë©´ 'ì „êµ­')
4. due_date: ì‹ ì²­ ë§ˆê°ì¼ (YYYY-MM-DD í˜•ì‹, ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ null)

ì‘ë‹µ í˜•ì‹ (JSON):
{
    "min_gpa": 3.0,
    "max_income": 8,
    "residence": "ì„œìš¸",
    "due_date": "2026-01-31"
}

ì£¼ì˜ì‚¬í•­:
- í•™ì ì´ ëª…ì‹œë˜ì§€ ì•Šì•˜ìœ¼ë©´ 0.0
- ì†Œë“ë¶„ìœ„ê°€ ëª…ì‹œë˜ì§€ ì•Šì•˜ìœ¼ë©´ 99
- ê±°ì£¼ì§€ê°€ ëª…ì‹œë˜ì§€ ì•Šì•˜ìœ¼ë©´ "ì „êµ­"
- ë§ˆê°ì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ null
- ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•  ê²ƒ"""

            user_prompt = f"""ì œëª©: {title}

ë‚´ìš©:
{content[:3000]}  # í† í° ì œí•œì„ ìœ„í•´ 3000ìë¡œ ì œí•œ

ìœ„ ì¥í•™ê¸ˆ ê³µê³ ë¥¼ ë¶„ì„í•˜ì—¬ min_gpa, max_income, residence, due_dateë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”."""

            # GPT-4o-mini API í˜¸ì¶œ
            response = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.1,
                max_tokens=500
            )
            
            # ì‘ë‹µ íŒŒì‹±
            result = json.loads(response.choices[0].message.content)
            
            # ë°ì´í„° ê²€ì¦ ë° ì •ì œ
            analyzed_data = {
                'min_gpa': float(result.get('min_gpa', 0.0)),
                'max_income': int(result.get('max_income', 99)),
                'residence': result.get('residence', 'ì „êµ­'),
                'due_date': result.get('due_date')
            }
            
            # due_date ìœ íš¨ì„± ê²€ì‚¬
            if analyzed_data['due_date']:
                try:
                    datetime.strptime(analyzed_data['due_date'], '%Y-%m-%d')
                except ValueError:
                    analyzed_data['due_date'] = None
            
            print(f"  âœ… ë¶„ì„ ì™„ë£Œ: {analyzed_data}")
            return analyzed_data
            
        except Exception as e:
            print(f"  âŒ GPT ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None


class SupabaseManager:
    """Supabase ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬"""
    
    @staticmethod
    def insert_scholarship(scholarship_data: Dict) -> bool:
        """
        ì¥í•™ê¸ˆ ë°ì´í„°ë¥¼ Supabaseì— ì‚½ì…í•©ë‹ˆë‹¤.
        
        Args:
            scholarship_data: ì‚½ì…í•  ì¥í•™ê¸ˆ ë°ì´í„°
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            print(f"  ğŸ’¾ DB ì €ì¥ ì¤‘: {scholarship_data['title'][:30]}...")
            
            # ì¤‘ë³µ ì²´í¬ (link ê¸°ì¤€)
            existing = supabase.table('scholarships') \
                .select('id') \
                .eq('link', scholarship_data['link']) \
                .execute()
            
            if existing.data:
                print(f"  âš ï¸  ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê³µê³ ì…ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                return False
            
            # ë°ì´í„° ì‚½ì…
            result = supabase.table('scholarships').insert(scholarship_data).execute()
            
            if result.data:
                print(f"  âœ… ì €ì¥ ì™„ë£Œ! ID: {result.data[0]['id']}")
                return True
            else:
                print(f"  âŒ ì €ì¥ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            print(f"  âŒ DB ì˜¤ë¥˜: {e}")
            return False
    
    @staticmethod
    def get_statistics() -> Dict:
        """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ"""
        try:
            result = supabase.table('scholarships').select('*', count='exact').execute()
            total = result.count
            
            # í™œì„± ì¥í•™ê¸ˆ (ë§ˆê°ì¼ ì§€ë‚˜ì§€ ì•Šì€ ê²ƒ)
            active_result = supabase.table('scholarships') \
                .select('*', count='exact') \
                .gte('due_date', datetime.now().strftime('%Y-%m-%d')) \
                .execute()
            active = active_result.count
            
            return {
                'total': total,
                'active': active,
                'expired': total - active
            }
        except Exception as e:
            print(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'total': 0, 'active': 0, 'expired': 0}


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ“ ì¥í•™ê¸ˆ í¬ë¡¤ëŸ¬ ì‹œì‘")
    print("=" * 60)
    
    # ì´ˆê¸° í†µê³„
    print("\nğŸ“Š í˜„ì¬ DB ìƒíƒœ:")
    stats = SupabaseManager.get_statistics()
    print(f"  - ì „ì²´ ì¥í•™ê¸ˆ: {stats['total']}ê°œ")
    print(f"  - í™œì„± ì¥í•™ê¸ˆ: {stats['active']}ê°œ")
    print(f"  - ë§Œë£Œ ì¥í•™ê¸ˆ: {stats['expired']}ê°œ\n")
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = ScholarshipCrawler()
    
    # ì¥í•™ê¸ˆ ëª©ë¡ í¬ë¡¤ë§
    scholarships = crawler.crawl_scholarship_list(TARGET_URL)
    
    if not scholarships:
        print("âŒ í¬ë¡¤ë§í•  ì¥í•™ê¸ˆì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nğŸ”„ ì´ {len(scholarships)}ê°œì˜ ì¥í•™ê¸ˆì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n")
    
    # ê° ì¥í•™ê¸ˆ ì²˜ë¦¬
    success_count = 0
    fail_count = 0
    
    for idx, scholarship in enumerate(scholarships, 1):
        print(f"\n[{idx}/{len(scholarships)}] {scholarship['title']}")
        print("-" * 60)
        
        # ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
        detail_content = crawler.crawl_scholarship_detail(scholarship['link'])
        
        if not detail_content:
            print("  âš ï¸  ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            fail_count += 1
            time.sleep(DELAY_SECONDS)
            continue
        
        # GPTë¡œ ë¶„ì„
        analyzed = GPTAnalyzer.analyze_scholarship(
            scholarship['title'],
            detail_content
        )
        
        if not analyzed:
            print("  âš ï¸  ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            fail_count += 1
            time.sleep(DELAY_SECONDS)
            continue
        
        # due_dateê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì • (ì˜ˆ: 3ê°œì›” í›„)
        if not analyzed['due_date']:
            from datetime import timedelta
            default_due = datetime.now() + timedelta(days=90)
            analyzed['due_date'] = default_due.strftime('%Y-%m-%d')
            print(f"  âš ï¸  ë§ˆê°ì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ ì„¤ì •: {analyzed['due_date']}")
        
        # DBì— ì €ì¥í•  ë°ì´í„° êµ¬ì„±
        scholarship_data = {
            'title': scholarship['title'],
            'link': scholarship['link'],
            'due_date': analyzed['due_date'],
            'min_gpa': analyzed['min_gpa'],
            'max_income': analyzed['max_income'],
            'residence': analyzed['residence']
        }
        
        # Supabaseì— ì €ì¥
        if SupabaseManager.insert_scholarship(scholarship_data):
            success_count += 1
        else:
            fail_count += 1
        
        # ìš”ì²­ ê°„ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
        time.sleep(DELAY_SECONDS)
    
    # ìµœì¢… í†µê³„
    print("\n" + "=" * 60)
    print("âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
    print("=" * 60)
    print(f"  - ì„±ê³µ: {success_count}ê°œ")
    print(f"  - ì‹¤íŒ¨: {fail_count}ê°œ")
    print(f"  - ì „ì²´: {len(scholarships)}ê°œ\n")
    
    # ìµœì‹  DB ìƒíƒœ
    print("ğŸ“Š ìµœì¢… DB ìƒíƒœ:")
    stats = SupabaseManager.get_statistics()
    print(f"  - ì „ì²´ ì¥í•™ê¸ˆ: {stats['total']}ê°œ")
    print(f"  - í™œì„± ì¥í•™ê¸ˆ: {stats['active']}ê°œ")
    print(f"  - ë§Œë£Œ ì¥í•™ê¸ˆ: {stats['expired']}ê°œ\n")


if __name__ == "__main__":
    main()

