"""
ì¥í•™ê¸ˆ ì •ë³´ í¬ë¡¤ë§ ë° ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ (ì´ë¯¸ì§€ ê¸°ë°˜ - GPT-4o Vision)
í•™êµ ê³µì§€ì‚¬í•­ì´ ì´ë¯¸ì§€ë¡œ ë˜ì–´ ìˆëŠ” ê²½ìš°ë¥¼ ìœ„í•œ ë²„ì „
"""

import os
import json
import time
import base64
import re
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from urllib.parse import urljoin, urlparse
from dotenv import load_dotenv

import requests
from bs4 import BeautifulSoup
from openai import OpenAI
from supabase import create_client, Client

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_KEY")
)

# ì„¤ì •ê°’
TARGET_URL = os.getenv("TARGET_URL", "https://web.kangnam.ac.kr")
BASE_DOMAIN = "https://web.kangnam.ac.kr"
MAX_PAGES = int(os.getenv("MAX_PAGES", "10"))
DELAY_SECONDS = int(os.getenv("DELAY_SECONDS", "2"))


class ScholarshipCrawler:
    """ì¥í•™ê¸ˆ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ (ì´ë¯¸ì§€ ê¸°ë°˜)"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        })
        self.processed_links = set()
    
    def crawl_scholarship_list(self, url: str) -> List[Dict]:
        """
        ì¥í•™ê¸ˆ ê²Œì‹œíŒ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ê³µê³  ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        Args:
            url: í¬ë¡¤ë§í•  ê²Œì‹œíŒ URL
            
        Returns:
            ì¥í•™ê¸ˆ ê³µê³  ë¦¬ìŠ¤íŠ¸ [{'title': str, 'link': str, 'date': str}, ...]
        """
        try:
            print(f"ğŸ“¡ í¬ë¡¤ë§ ì‹œì‘: {url}")
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'lxml')
            scholarships = []
            
            # class="detailLink"ë¥¼ ê°€ì§„ ëª¨ë“  ë§í¬ ì°¾ê¸°
            detail_links = soup.find_all('a', class_='detailLink')
            
            print(f"âœ… {len(detail_links)}ê°œì˜ ê³µê³ ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
            
            for link in detail_links:
                title = link.get_text(strip=True)
                
                # href ì†ì„±ì—ì„œ ë§í¬ ì¶”ì¶œ
                href = link.get('href', '')
                
                # data-paramsì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
                data_params = link.get('data-params', '')
                
                # ìƒì„¸ í˜ì´ì§€ URL êµ¬ì„±
                if href and href != '#':
                    detail_url = self._build_full_url(BASE_DOMAIN, href)
                elif data_params:
                    # data-paramsì—ì„œ URL íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                    detail_url = self._extract_detail_url_from_params(data_params)
                else:
                    continue
                
                if detail_url and detail_url not in self.processed_links:
                    scholarships.append({
                        'title': title,
                        'link': detail_url,
                    })
                    self.processed_links.add(detail_url)
            
            return scholarships
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return []
    
    def crawl_scholarship_detail(self, url: str) -> Tuple[Optional[str], Optional[str]]:
        """
        ì¥í•™ê¸ˆ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì´ë¯¸ì§€ URLê³¼ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        Args:
            url: ìƒì„¸ í˜ì´ì§€ URL
            
        Returns:
            (image_url, text_content) íŠœí”Œ
        """
        try:
            print(f"  ğŸ“„ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§: {url}")
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            # ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸° (.tbl_view í´ë˜ìŠ¤)
            content_div = soup.find('div', class_='tbl_view')
            
            if not content_div:
                # ëŒ€ì²´ í´ë˜ìŠ¤ëª… ì‹œë„
                content_div = soup.find('div', class_=re.compile(r'content|view|detail|article', re.I))
            
            if not content_div:
                print(f"  âš ï¸  ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None, None
            
            # 1. ì´ë¯¸ì§€ ì°¾ê¸° (ìš°ì„ ìˆœìœ„)
            image_url = None
            img_tag = content_div.find('img')
            
            if img_tag:
                img_src = img_tag.get('src', '')
                if img_src:
                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    image_url = self._build_full_url(BASE_DOMAIN, img_src)
                    print(f"  ğŸ–¼ï¸  ì´ë¯¸ì§€ ë°œê²¬: {image_url}")
            
            # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë°±ì—…ìš©)
            text_content = content_div.get_text(strip=True, separator='\n')
            
            return image_url, text_content
            
        except Exception as e:
            print(f"  âŒ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None, None
    
    def download_image_as_base64(self, image_url: str) -> Optional[str]:
        """
        ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ Base64 ë¬¸ìì—´ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤.
        
        Args:
            image_url: ì´ë¯¸ì§€ URL
            
        Returns:
            Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë¬¸ìì—´
        """
        try:
            print(f"  ğŸ“¥ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘: {image_url}")
            
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            response = self.session.get(image_url, timeout=15)
            response.raise_for_status()
            
            # Base64 ì¸ì½”ë”©
            image_data = base64.b64encode(response.content).decode('utf-8')
            
            # ì´ë¯¸ì§€ íƒ€ì… ì¶”ì¶œ
            content_type = response.headers.get('Content-Type', 'image/jpeg')
            
            print(f"  âœ… ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ({len(image_data)} bytes)")
            
            return f"data:{content_type};base64,{image_data}"
            
        except Exception as e:
            print(f"  âŒ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _build_full_url(self, base_url: str, path: str) -> str:
        """ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜"""
        if path.startswith('http'):
            return path
        return urljoin(base_url, path)
    
    def _extract_detail_url_from_params(self, data_params: str) -> Optional[str]:
        """data-paramsì—ì„œ ìƒì„¸ í˜ì´ì§€ URL ì¶”ì¶œ"""
        try:
            # data-paramsê°€ JSON í˜•íƒœì¸ ê²½ìš°
            params = json.loads(data_params.replace('&quot;', '"'))
            
            # ê°•ë‚¨ëŒ€ ê²Œì‹œíŒ êµ¬ì¡°ì— ë§ê²Œ URL êµ¬ì„±
            if 'encMenuBoardSeq' in params:
                board_seq = params['encMenuBoardSeq']
                # ì‹¤ì œ ê²Œì‹œíŒ ìƒì„¸ URL íŒ¨í„´ì— ë§ê²Œ ìˆ˜ì • í•„ìš”
                return f"{BASE_DOMAIN}/board/view?seq={board_seq}"
            
            return None
        except Exception as e:
            print(f"  âš ï¸  data-params íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None


class GPTVisionAnalyzer:
    """GPT-4o Visionì„ ì‚¬ìš©í•œ ì¥í•™ê¸ˆ ì •ë³´ ë¶„ì„"""
    
    @staticmethod
    def analyze_with_image(title: str, image_base64: str) -> Optional[Dict]:
        """
        GPT-4o Visionìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
        
        Args:
            title: ì¥í•™ê¸ˆ ì œëª©
            image_base64: Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            print(f"  ğŸ¤– GPT-4o Vision ë¶„ì„ ì¤‘...")
            
            # GPT-4o Vision API í˜¸ì¶œ
            response = openai_client.chat.completions.create(
                model="gpt-4o",  # Vision ì§€ì› ëª¨ë¸
                messages=[
                    {
                        "role": "system",
                        "content": """ë‹¹ì‹ ì€ ì¥í•™ê¸ˆ ê³µê³  ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì´ë¯¸ì§€ì—ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSON í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”.

ì¶”ì¶œí•  ì •ë³´:
1. min_gpa: ìµœì†Œ í•™ì  ìš”êµ¬ì‚¬í•­ (ì—†ìœ¼ë©´ 0.0)
2. max_income: ì†Œë“ë¶„ìœ„ ìƒí•œì„  (0~10, ì œí•œ ì—†ìœ¼ë©´ 99)
3. residence: ê±°ì£¼ì§€ ì œí•œ (ì˜ˆ: 'ì„œìš¸', 'ê²½ê¸°', 'ì „êµ­' ë“±, ì œí•œ ì—†ìœ¼ë©´ 'ì „êµ­')
4. due_date: ì‹ ì²­ ë§ˆê°ì¼ (YYYY-MM-DD í˜•ì‹, ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ null)

ì‘ë‹µ í˜•ì‹ (JSON):
{
    "min_gpa": 3.0,
    "max_income": 8,
    "residence": "ì„œìš¸",
    "due_date": "2026-01-31"
}

ì£¼ì˜ì‚¬í•­:
- í•™ì ì´ ëª…ì‹œë˜ì§€ ì•Šì•˜ìœ¼ë©´ 0.0
- ì†Œë“ë¶„ìœ„ê°€ ëª…ì‹œë˜ì§€ ì•Šì•˜ìœ¼ë©´ 99
- ê±°ì£¼ì§€ê°€ ëª…ì‹œë˜ì§€ ì•Šì•˜ìœ¼ë©´ "ì „êµ­"
- ë§ˆê°ì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ null
- ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•  ê²ƒ
- ì´ë¯¸ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ì •í™•í•˜ê²Œ ì½ì–´ì„œ ì¶”ì¶œí•  ê²ƒ"""
                    },
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": f"""ì œëª©: {title}

ìœ„ ì¥í•™ê¸ˆ ê³µê³  ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ min_gpa, max_income, residence, due_dateë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
ì´ë¯¸ì§€ ë‚´ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì£¼ì˜ ê¹Šê²Œ ì½ì–´ì£¼ì„¸ìš”."""
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": image_base64,
                                    "detail": "high"  # ê³ í•´ìƒë„ ë¶„ì„
                                }
                            }
                        ]
                    }
                ],
                response_format={"type": "json_object"},
                temperature=0.1,
                max_tokens=500
            )
            
            # ì‘ë‹µ íŒŒì‹±
            result = json.loads(response.choices[0].message.content)
            
            # ë°ì´í„° ê²€ì¦ ë° ì •ì œ
            analyzed_data = {
                'min_gpa': float(result.get('min_gpa', 0.0)),
                'max_income': int(result.get('max_income', 99)),
                'residence': result.get('residence', 'ì „êµ­'),
                'due_date': result.get('due_date')
            }
            
            # due_date ìœ íš¨ì„± ê²€ì‚¬
            if analyzed_data['due_date']:
                try:
                    datetime.strptime(analyzed_data['due_date'], '%Y-%m-%d')
                except ValueError:
                    analyzed_data['due_date'] = None
            
            print(f"  âœ… ë¶„ì„ ì™„ë£Œ: {analyzed_data}")
            return analyzed_data
            
        except Exception as e:
            print(f"  âŒ GPT Vision ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None
    
    @staticmethod
    def analyze_with_text(title: str, content: str) -> Optional[Dict]:
        """
        í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ì„ (ì´ë¯¸ì§€ê°€ ì—†ì„ ë•Œ í´ë°±)
        
        Args:
            title: ì¥í•™ê¸ˆ ì œëª©
            content: ë³¸ë¬¸ í…ìŠ¤íŠ¸
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            print(f"  ğŸ¤– GPT-4o í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘...")
            
            system_prompt = """ë‹¹ì‹ ì€ ì¥í•™ê¸ˆ ê³µê³ ë¬¸ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì¥í•™ê¸ˆ ê³µê³ ì—ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSON í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”.

ì¶”ì¶œí•  ì •ë³´:
1. min_gpa: ìµœì†Œ í•™ì  ìš”êµ¬ì‚¬í•­ (ì—†ìœ¼ë©´ 0.0)
2. max_income: ì†Œë“ë¶„ìœ„ ìƒí•œì„  (0~10, ì œí•œ ì—†ìœ¼ë©´ 99)
3. residence: ê±°ì£¼ì§€ ì œí•œ (ì˜ˆ: 'ì„œìš¸', 'ê²½ê¸°', 'ì „êµ­' ë“±, ì œí•œ ì—†ìœ¼ë©´ 'ì „êµ­')
4. due_date: ì‹ ì²­ ë§ˆê°ì¼ (YYYY-MM-DD í˜•ì‹, ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ null)

ì‘ë‹µ í˜•ì‹ (JSON):
{
    "min_gpa": 3.0,
    "max_income": 8,
    "residence": "ì„œìš¸",
    "due_date": "2026-01-31"
}

ì£¼ì˜ì‚¬í•­:
- í•™ì ì´ ëª…ì‹œë˜ì§€ ì•Šì•˜ìœ¼ë©´ 0.0
- ì†Œë“ë¶„ìœ„ê°€ ëª…ì‹œë˜ì§€ ì•Šì•˜ìœ¼ë©´ 99
- ê±°ì£¼ì§€ê°€ ëª…ì‹œë˜ì§€ ì•Šì•˜ìœ¼ë©´ "ì „êµ­"
- ë§ˆê°ì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ null
- ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•  ê²ƒ"""

            user_prompt = f"""ì œëª©: {title}

ë‚´ìš©:
{content[:4000]}  # í† í° ì œí•œ

ìœ„ ì¥í•™ê¸ˆ ê³µê³ ë¥¼ ë¶„ì„í•˜ì—¬ min_gpa, max_income, residence, due_dateë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”."""

            # GPT-4o API í˜¸ì¶œ
            response = openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.1,
                max_tokens=500
            )
            
            # ì‘ë‹µ íŒŒì‹±
            result = json.loads(response.choices[0].message.content)
            
            # ë°ì´í„° ê²€ì¦ ë° ì •ì œ
            analyzed_data = {
                'min_gpa': float(result.get('min_gpa', 0.0)),
                'max_income': int(result.get('max_income', 99)),
                'residence': result.get('residence', 'ì „êµ­'),
                'due_date': result.get('due_date')
            }
            
            # due_date ìœ íš¨ì„± ê²€ì‚¬
            if analyzed_data['due_date']:
                try:
                    datetime.strptime(analyzed_data['due_date'], '%Y-%m-%d')
                except ValueError:
                    analyzed_data['due_date'] = None
            
            print(f"  âœ… ë¶„ì„ ì™„ë£Œ: {analyzed_data}")
            return analyzed_data
            
        except Exception as e:
            print(f"  âŒ GPT í…ìŠ¤íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None


class SupabaseManager:
    """Supabase ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬"""
    
    @staticmethod
    def insert_scholarship(scholarship_data: Dict) -> bool:
        """
        ì¥í•™ê¸ˆ ë°ì´í„°ë¥¼ Supabaseì— ì‚½ì…í•©ë‹ˆë‹¤.
        
        Args:
            scholarship_data: ì‚½ì…í•  ì¥í•™ê¸ˆ ë°ì´í„°
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            print(f"  ğŸ’¾ DB ì €ì¥ ì¤‘: {scholarship_data['title'][:30]}...")
            
            # ì¤‘ë³µ ì²´í¬ (link ê¸°ì¤€)
            existing = supabase.table('scholarships') \
                .select('id') \
                .eq('link', scholarship_data['link']) \
                .execute()
            
            if existing.data:
                print(f"  âš ï¸  ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê³µê³ ì…ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                return False
            
            # ë°ì´í„° ì‚½ì…
            result = supabase.table('scholarships').insert(scholarship_data).execute()
            
            if result.data:
                print(f"  âœ… ì €ì¥ ì™„ë£Œ! ID: {result.data[0]['id']}")
                return True
            else:
                print(f"  âŒ ì €ì¥ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            print(f"  âŒ DB ì˜¤ë¥˜: {e}")
            return False
    
    @staticmethod
    def get_statistics() -> Dict:
        """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ"""
        try:
            result = supabase.table('scholarships').select('*', count='exact').execute()
            total = result.count
            
            # í™œì„± ì¥í•™ê¸ˆ (ë§ˆê°ì¼ ì§€ë‚˜ì§€ ì•Šì€ ê²ƒ)
            active_result = supabase.table('scholarships') \
                .select('*', count='exact') \
                .gte('due_date', datetime.now().strftime('%Y-%m-%d')) \
                .execute()
            active = active_result.count
            
            return {
                'total': total,
                'active': active,
                'expired': total - active
            }
        except Exception as e:
            print(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'total': 0, 'active': 0, 'expired': 0}


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 70)
    print("ğŸ“ ì¥í•™ê¸ˆ í¬ë¡¤ëŸ¬ ì‹œì‘ (ì´ë¯¸ì§€ ê¸°ë°˜ - GPT-4o Vision)")
    print("=" * 70)
    
    # ì´ˆê¸° í†µê³„
    print("\nğŸ“Š í˜„ì¬ DB ìƒíƒœ:")
    stats = SupabaseManager.get_statistics()
    print(f"  - ì „ì²´ ì¥í•™ê¸ˆ: {stats['total']}ê°œ")
    print(f"  - í™œì„± ì¥í•™ê¸ˆ: {stats['active']}ê°œ")
    print(f"  - ë§Œë£Œ ì¥í•™ê¸ˆ: {stats['expired']}ê°œ\n")
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = ScholarshipCrawler()
    
    # ì¥í•™ê¸ˆ ëª©ë¡ í¬ë¡¤ë§
    scholarships = crawler.crawl_scholarship_list(TARGET_URL)
    
    if not scholarships:
        print("âŒ í¬ë¡¤ë§í•  ì¥í•™ê¸ˆì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nğŸ”„ ì´ {len(scholarships)}ê°œì˜ ì¥í•™ê¸ˆì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n")
    
    # ê° ì¥í•™ê¸ˆ ì²˜ë¦¬
    success_count = 0
    fail_count = 0
    image_count = 0
    text_count = 0
    
    for idx, scholarship in enumerate(scholarships, 1):
        print(f"\n[{idx}/{len(scholarships)}] {scholarship['title']}")
        print("-" * 70)
        
        # ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ (ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸)
        image_url, text_content = crawler.crawl_scholarship_detail(scholarship['link'])
        
        if not image_url and not text_content:
            print("  âš ï¸  ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            fail_count += 1
            time.sleep(DELAY_SECONDS)
            continue
        
        analyzed = None
        
        # ì „ëµ 1: ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ Visionìœ¼ë¡œ ë¶„ì„ (ìš°ì„ ìˆœìœ„)
        if image_url:
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° Base64 ì¸ì½”ë”©
            image_base64 = crawler.download_image_as_base64(image_url)
            
            if image_base64:
                # GPT-4o Vision ë¶„ì„
                analyzed = GPTVisionAnalyzer.analyze_with_image(
                    scholarship['title'],
                    image_base64
                )
                
                if analyzed:
                    image_count += 1
                    print(f"  ğŸ“¸ ì´ë¯¸ì§€ ê¸°ë°˜ ë¶„ì„ ì„±ê³µ")
        
        # ì „ëµ 2: ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨ or ì´ë¯¸ì§€ ì—†ìŒ â†’ í…ìŠ¤íŠ¸ ë¶„ì„ (í´ë°±)
        if not analyzed and text_content:
            print(f"  ğŸ“ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ ì „í™˜...")
            analyzed = GPTVisionAnalyzer.analyze_with_text(
                scholarship['title'],
                text_content
            )
            
            if analyzed:
                text_count += 1
                print(f"  ğŸ“ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ì„ ì„±ê³µ")
        
        # ë¶„ì„ ì‹¤íŒ¨
        if not analyzed:
            print("  âš ï¸  ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            fail_count += 1
            time.sleep(DELAY_SECONDS)
            continue
        
        # due_dateê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
        if not analyzed['due_date']:
            from datetime import timedelta
            default_due = datetime.now() + timedelta(days=90)
            analyzed['due_date'] = default_due.strftime('%Y-%m-%d')
            print(f"  âš ï¸  ë§ˆê°ì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ ì„¤ì •: {analyzed['due_date']}")
        
        # DBì— ì €ì¥í•  ë°ì´í„° êµ¬ì„±
        scholarship_data = {
            'title': scholarship['title'],
            'link': scholarship['link'],
            'due_date': analyzed['due_date'],
            'min_gpa': analyzed['min_gpa'],
            'max_income': analyzed['max_income'],
            'residence': analyzed['residence']
        }
        
        # Supabaseì— ì €ì¥
        if SupabaseManager.insert_scholarship(scholarship_data):
            success_count += 1
        else:
            fail_count += 1
        
        # ìš”ì²­ ê°„ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€ + API Rate Limit)
        time.sleep(DELAY_SECONDS)
    
    # ìµœì¢… í†µê³„
    print("\n" + "=" * 70)
    print("âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
    print("=" * 70)
    print(f"  - ì„±ê³µ: {success_count}ê°œ")
    print(f"  - ì‹¤íŒ¨: {fail_count}ê°œ")
    print(f"  - ì „ì²´: {len(scholarships)}ê°œ")
    print(f"\n  ğŸ“Š ë¶„ì„ ë°©ë²•:")
    print(f"  - ì´ë¯¸ì§€ ê¸°ë°˜ (Vision): {image_count}ê°œ")
    print(f"  - í…ìŠ¤íŠ¸ ê¸°ë°˜: {text_count}ê°œ\n")
    
    # ìµœì‹  DB ìƒíƒœ
    print("ğŸ“Š ìµœì¢… DB ìƒíƒœ:")
    stats = SupabaseManager.get_statistics()
    print(f"  - ì „ì²´ ì¥í•™ê¸ˆ: {stats['total']}ê°œ")
    print(f"  - í™œì„± ì¥í•™ê¸ˆ: {stats['active']}ê°œ")
    print(f"  - ë§Œë£Œ ì¥í•™ê¸ˆ: {stats['expired']}ê°œ\n")


if __name__ == "__main__":
    main()

