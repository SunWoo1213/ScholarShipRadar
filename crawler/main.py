"""
ì¥í•™ê¸ˆ ë©”ì¸ í¬ë¡¤ëŸ¬ (GPT-4o Vision ì „ìš©)
í†µì´ë¯¸ì§€ ê³µê³ ë¥¼ ë¶„ì„í•˜ì—¬ Supabaseì— ì €ì¥í•©ë‹ˆë‹¤.
"""

import os
import json
import base64
import time
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from urllib.parse import urljoin
from dotenv import load_dotenv

import requests
from bs4 import BeautifulSoup
from openai import OpenAI
from supabase import create_client, Client

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_KEY")
)

# ì„¤ì •
TARGET_URL = os.getenv("TARGET_URL", "https://web.kangnam.ac.kr/board/scholarship")
BASE_DOMAIN = os.getenv("BASE_DOMAIN", "https://web.kangnam.ac.kr")
MAX_ITEMS = int(os.getenv("MAX_ITEMS", "50"))
DELAY_SECONDS = int(os.getenv("DELAY_SECONDS", "3"))


class ScholarshipCrawler:
    """ì¥í•™ê¸ˆ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        })
        
    def crawl_list(self, url: str) -> List[Dict]:
        """
        ê²Œì‹œíŒ ëª©ë¡ì—ì„œ ì œëª©, ë§í¬ ì¶”ì¶œ
        """
        try:
            print(f"ğŸ“¡ ê²Œì‹œíŒ í¬ë¡¤ë§: {url}")
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            items = []
            
            # ë§í¬ ì°¾ê¸° (ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
            links = soup.find_all('a', class_='detailLink')
            
            if not links:
                # ëŒ€ì²´ ë°©ë²•: href ì†ì„±ì´ ìˆëŠ” ëª¨ë“  a íƒœê·¸
                links = soup.select('div.board-list a, table.board-list a, ul.board-list a')
            
            print(f"âœ… {len(links)}ê°œ ê³µê³  ë°œê²¬")
            
            for link in links[:MAX_ITEMS]:
                title = link.get_text(strip=True)
                href = link.get('href', '')
                
                if not href or href == '#':
                    # data-paramsì—ì„œ URL ì¶”ì¶œ ì‹œë„
                    data_params = link.get('data-params', '')
                    if data_params:
                        detail_url = self._extract_url_from_params(data_params)
                    else:
                        continue
                else:
                    detail_url = self._build_full_url(href)
                
                if detail_url and title:
                    items.append({
                        'title': title,
                        'link': detail_url
                    })
            
            return items
            
        except Exception as e:
            print(f"âŒ ëª©ë¡ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []
    
    def crawl_detail(self, url: str) -> Tuple[Optional[str], Optional[str], bool]:
        """
        ìƒì„¸ í˜ì´ì§€ì—ì„œ ì´ë¯¸ì§€ URL ë˜ëŠ” í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Returns:
            (image_url, text_content, is_image)
        """
        try:
            print(f"  ğŸ“„ ìƒì„¸ í˜ì´ì§€: {url}")
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
            content_div = soup.find('div', class_='tbl_view')
            
            if not content_div:
                # ëŒ€ì²´ í´ë˜ìŠ¤ëª…ë“¤ ì‹œë„
                for class_name in ['view-content', 'board-content', 'content', 'article-body']:
                    content_div = soup.find('div', class_=class_name)
                    if content_div:
                        break
            
            if not content_div:
                print(f"  âš ï¸  ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None, None, False
            
            # 1. ì´ë¯¸ì§€ ì°¾ê¸° (ìš°ì„ )
            img_tag = content_div.find('img')
            
            if img_tag:
                img_src = img_tag.get('src', '')
                if img_src:
                    image_url = self._build_full_url(img_src)
                    print(f"  ğŸ–¼ï¸  ì´ë¯¸ì§€ ë°œê²¬")
                    return image_url, None, True
            
            # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ (í´ë°±)
            text_content = content_div.get_text(strip=True, separator='\n')
            if text_content:
                print(f"  ğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ({len(text_content)}ì)")
                return None, text_content, False
            
            return None, None, False
            
        except Exception as e:
            print(f"  âŒ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None, None, False
    
    def download_image_as_base64(self, image_url: str) -> Optional[str]:
        """
        ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©
        """
        try:
            print(f"  ğŸ“¥ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            response = self.session.get(image_url, timeout=15)
            response.raise_for_status()
            
            image_data = base64.b64encode(response.content).decode('utf-8')
            content_type = response.headers.get('Content-Type', 'image/jpeg')
            
            print(f"  âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ({len(image_data)//1024}KB)")
            
            return f"data:{content_type};base64,{image_data}"
            
        except Exception as e:
            print(f"  âŒ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _build_full_url(self, path: str) -> str:
        """ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜"""
        if path.startswith('http'):
            return path
        return urljoin(BASE_DOMAIN, path)
    
    def _extract_url_from_params(self, data_params: str) -> Optional[str]:
        """data-paramsì—ì„œ URL ì¶”ì¶œ"""
        try:
            params = json.loads(data_params.replace('&quot;', '"'))
            if 'encMenuBoardSeq' in params:
                return f"{BASE_DOMAIN}/board/view?seq={params['encMenuBoardSeq']}"
            return None
        except:
            return None


class GPTAnalyzer:
    """GPT-4o Vision/Text ë¶„ì„ê¸°"""
    
    @staticmethod
    def analyze_image(title: str, image_base64: str) -> Optional[Dict]:
        """
        GPT-4o Visionìœ¼ë¡œ ì´ë¯¸ì§€ ë¶„ì„
        """
        try:
            print(f"  ğŸ¤– GPT-4o Vision ë¶„ì„ ì¤‘...")
            
            response = openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": """ë‹¹ì‹ ì€ ì¥í•™ê¸ˆ ê³µê³  ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ëŠ” AIì…ë‹ˆë‹¤.
ì´ë¯¸ì§€ì—ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSONìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:

1. min_gpa: ìµœì†Œ í•™ì  ìš”êµ¬ì‚¬í•­ (ì˜ˆ: 3.0, ì—†ìœ¼ë©´ 0.0)
2. max_income: ì†Œë“ë¶„ìœ„ ìƒí•œì„  (0~10, ì œí•œ ì—†ìœ¼ë©´ 99)
3. residence: ê±°ì£¼ì§€ ì œí•œ (ì˜ˆ: "ì„œìš¸", "ê²½ê¸°ë„", ì—†ìœ¼ë©´ "ì „êµ­")
4. due_date: ë§ˆê°ì¼ (YYYY-MM-DD, ì—†ìœ¼ë©´ null)

ì‘ë‹µ í˜•ì‹:
{
    "min_gpa": 3.0,
    "max_income": 8,
    "residence": "ì „êµ­",
    "due_date": "2026-01-31"
}

ì£¼ì˜: ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”."""
                    },
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": f"ì œëª©: {title}\n\nìœ„ ì¥í•™ê¸ˆ ê³µê³  ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": image_base64,
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                response_format={"type": "json_object"},
                temperature=0.1,
                max_tokens=500
            )
            
            result = json.loads(response.choices[0].message.content)
            
            # ë°ì´í„° ê²€ì¦
            analyzed = {
                'min_gpa': float(result.get('min_gpa', 0.0)),
                'max_income': int(result.get('max_income', 99)),
                'residence': result.get('residence', 'ì „êµ­'),
                'due_date': result.get('due_date')
            }
            
            # ë‚ ì§œ ìœ íš¨ì„± ê²€ì‚¬
            if analyzed['due_date']:
                try:
                    datetime.strptime(analyzed['due_date'], '%Y-%m-%d')
                except ValueError:
                    analyzed['due_date'] = None
            
            print(f"  âœ… ë¶„ì„ ì™„ë£Œ: {analyzed}")
            return analyzed
            
        except Exception as e:
            print(f"  âŒ Vision ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None
    
    @staticmethod
    def analyze_text(title: str, content: str) -> Optional[Dict]:
        """
        GPT-4oë¡œ í…ìŠ¤íŠ¸ ë¶„ì„ (í´ë°±)
        """
        try:
            print(f"  ğŸ¤– GPT-4o í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘...")
            
            response = openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": """ë‹¹ì‹ ì€ ì¥í•™ê¸ˆ ê³µê³ ë¬¸ì„ ë¶„ì„í•˜ëŠ” AIì…ë‹ˆë‹¤.
í…ìŠ¤íŠ¸ì—ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSONìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:

1. min_gpa: ìµœì†Œ í•™ì  (ì—†ìœ¼ë©´ 0.0)
2. max_income: ì†Œë“ë¶„ìœ„ ìƒí•œì„  (0~10, ì—†ìœ¼ë©´ 99)
3. residence: ê±°ì£¼ì§€ (ì—†ìœ¼ë©´ "ì „êµ­")
4. due_date: ë§ˆê°ì¼ (YYYY-MM-DD, ì—†ìœ¼ë©´ null)

ì‘ë‹µ í˜•ì‹:
{
    "min_gpa": 2.5,
    "max_income": 5,
    "residence": "ì„œìš¸",
    "due_date": "2026-02-15"
}"""
                    },
                    {
                        "role": "user",
                        "content": f"ì œëª©: {title}\n\në‚´ìš©:\n{content[:3000]}\n\nìœ„ ê³µê³ ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."
                    }
                ],
                response_format={"type": "json_object"},
                temperature=0.1,
                max_tokens=500
            )
            
            result = json.loads(response.choices[0].message.content)
            
            analyzed = {
                'min_gpa': float(result.get('min_gpa', 0.0)),
                'max_income': int(result.get('max_income', 99)),
                'residence': result.get('residence', 'ì „êµ­'),
                'due_date': result.get('due_date')
            }
            
            if analyzed['due_date']:
                try:
                    datetime.strptime(analyzed['due_date'], '%Y-%m-%d')
                except ValueError:
                    analyzed['due_date'] = None
            
            print(f"  âœ… ë¶„ì„ ì™„ë£Œ: {analyzed}")
            return analyzed
            
        except Exception as e:
            print(f"  âŒ í…ìŠ¤íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None


class DatabaseManager:
    """Supabase ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬"""
    
    @staticmethod
    def upsert_scholarship(data: Dict) -> bool:
        """
        ì¥í•™ê¸ˆ ì •ë³´ë¥¼ Upsert (ë§í¬ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì‹œ ì—…ë°ì´íŠ¸)
        """
        try:
            print(f"  ğŸ’¾ DB ì €ì¥ ì¤‘...")
            
            # Upsert: linkê°€ ê°™ìœ¼ë©´ ì—…ë°ì´íŠ¸
            result = supabase.table('scholarships').upsert(
                data,
                on_conflict='link'
            ).execute()
            
            if result.data:
                print(f"  âœ… ì €ì¥ ì™„ë£Œ! ID: {result.data[0]['id']}")
                return True
            else:
                print(f"  âŒ ì €ì¥ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            print(f"  âŒ DB ì˜¤ë¥˜: {e}")
            return False
    
    @staticmethod
    def get_statistics() -> Dict:
        """í†µê³„ ì¡°íšŒ"""
        try:
            total = supabase.table('scholarships').select('*', count='exact').execute().count
            
            today = datetime.now().strftime('%Y-%m-%d')
            active = supabase.table('scholarships').select('*', count='exact').gte('due_date', today).execute().count
            
            return {
                'total': total,
                'active': active,
                'expired': total - active
            }
        except:
            return {'total': 0, 'active': 0, 'expired': 0}


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 80)
    print("ğŸ“ ì¥í•™ê¸ˆ ë©”ì¸ í¬ë¡¤ëŸ¬ (GPT-4o Vision)")
    print("=" * 80)
    
    # ì´ˆê¸° í†µê³„
    print("\nğŸ“Š í˜„ì¬ DB ìƒíƒœ:")
    stats = DatabaseManager.get_statistics()
    print(f"  ì „ì²´: {stats['total']}ê°œ | í™œì„±: {stats['active']}ê°œ | ë§Œë£Œ: {stats['expired']}ê°œ\n")
    
    # í¬ë¡¤ëŸ¬ ì‹œì‘
    crawler = ScholarshipCrawler()
    
    # 1. ëª©ë¡ í¬ë¡¤ë§
    items = crawler.crawl_list(TARGET_URL)
    
    if not items:
        print("âŒ í¬ë¡¤ë§í•  ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nğŸ”„ ì´ {len(items)}ê°œ ê³µê³ ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n")
    
    # í†µê³„
    success_count = 0
    fail_count = 0
    image_count = 0
    text_count = 0
    
    # 2. ê° ê³µê³  ì²˜ë¦¬
    for idx, item in enumerate(items, 1):
        print(f"\n[{idx}/{len(items)}] {item['title'][:50]}...")
        print("-" * 80)
        
        # ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
        image_url, text_content, is_image = crawler.crawl_detail(item['link'])
        
        if not image_url and not text_content:
            print("  âš ï¸  ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            fail_count += 1
            time.sleep(DELAY_SECONDS)
            continue
        
        analyzed = None
        
        # Vision ë¶„ì„ (ì´ë¯¸ì§€ê°€ ìˆì„ ë•Œ)
        if is_image and image_url:
            image_base64 = crawler.download_image_as_base64(image_url)
            
            if image_base64:
                analyzed = GPTAnalyzer.analyze_image(item['title'], image_base64)
                
                if analyzed:
                    image_count += 1
        
        # í…ìŠ¤íŠ¸ ë¶„ì„ (í´ë°±)
        if not analyzed and text_content:
            analyzed = GPTAnalyzer.analyze_text(item['title'], text_content)
            
            if analyzed:
                text_count += 1
                is_image = False
        
        # ë¶„ì„ ì‹¤íŒ¨
        if not analyzed:
            print("  âš ï¸  ë¶„ì„ ì‹¤íŒ¨")
            fail_count += 1
            time.sleep(DELAY_SECONDS)
            continue
        
        # ë§ˆê°ì¼ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ (3ê°œì›” í›„)
        if not analyzed['due_date']:
            default_due = datetime.now() + timedelta(days=90)
            analyzed['due_date'] = default_due.strftime('%Y-%m-%d')
            print(f"  âš ï¸  ë§ˆê°ì¼ ì—†ìŒ, ê¸°ë³¸ê°’: {analyzed['due_date']}")
        
        # DB ì €ì¥
        scholarship_data = {
            'title': item['title'],
            'link': item['link'],
            'due_date': analyzed['due_date'],
            'min_gpa': analyzed['min_gpa'],
            'max_income': analyzed['max_income'],
            'residence': analyzed['residence'],
            'is_image_content': is_image
        }
        
        if DatabaseManager.upsert_scholarship(scholarship_data):
            success_count += 1
        else:
            fail_count += 1
        
        # ë”œë ˆì´
        time.sleep(DELAY_SECONDS)
    
    # ìµœì¢… ê²°ê³¼
    print("\n" + "=" * 80)
    print("âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
    print("=" * 80)
    print(f"  ì„±ê³µ: {success_count}ê°œ | ì‹¤íŒ¨: {fail_count}ê°œ | ì „ì²´: {len(items)}ê°œ")
    print(f"\n  ğŸ“Š ë¶„ì„ ë°©ë²•:")
    print(f"    - ì´ë¯¸ì§€ (Vision): {image_count}ê°œ")
    print(f"    - í…ìŠ¤íŠ¸: {text_count}ê°œ\n")
    
    # ìµœì‹  í†µê³„
    print("ğŸ“Š ìµœì¢… DB ìƒíƒœ:")
    stats = DatabaseManager.get_statistics()
    print(f"  ì „ì²´: {stats['total']}ê°œ | í™œì„±: {stats['active']}ê°œ | ë§Œë£Œ: {stats['expired']}ê°œ\n")


if __name__ == "__main__":
    main()

